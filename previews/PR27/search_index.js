var documenterSearchIndex = {"docs":
[{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"EditURL = \"../literate/heat_equation_hypre.jl\"","category":"page"},{"location":"examples/heat_equation_hypre/#Distributed-Assembly-with-HYPRE.jl","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"","category":"section"},{"location":"examples/heat_equation_hypre/#Introduction","page":"Distributed Assembly with HYPRE.jl","title":"Introduction","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Now we want to solve the heat problem in parallel. To be specific, this example shows how to utilize process parallelism to assemble finite element matrices in parallel. This example presumes that the reader is familiar with solving the heat problem in serial with Ferrite.jl, as presented in the first example.","category":"page"},{"location":"examples/heat_equation_hypre/#Commented-Program","page":"Distributed Assembly with HYPRE.jl","title":"Commented Program","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Now we solve the problem in Ferrite. What follows is a program spliced with comments. The full program, without comments, can be found in the next section.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"First we load Ferrite, and some other packages we need","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"using FerriteDistributed\nusing HYPRE, Metis\n\nimport FerriteDistributed: getglobalgrid, global_comm, local_dof_range #TODO REMOVE THIS","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Launch MPI and HYPRE","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"MPI.Init()\nHYPRE.Init()","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"We start generating a simple grid with 10x10x10 hexahedral elements and distribute it across our processors using generate_distributed_grid.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"dgrid = generate_nod_grid(MPI.COMM_WORLD, Hexahedron, (10, 10, 10); partitioning_alg=FerriteDistributed.PartitioningAlgorithm.Metis(:RECURSIVE));\nnothing #hide","category":"page"},{"location":"examples/heat_equation_hypre/#Trial-and-test-functions","page":"Distributed Assembly with HYPRE.jl","title":"Trial and test functions","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Nothing changes here.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"ref = RefHexahedron\nip = Lagrange{ref, 2}()\nip_geo = Lagrange{ref, 1}()\nqr = QuadratureRule{ref}(3)\ncellvalues = CellValues(qr, ip, ip_geo);\nnothing #hide","category":"page"},{"location":"examples/heat_equation_hypre/#Degrees-of-freedom","page":"Distributed Assembly with HYPRE.jl","title":"Degrees of freedom","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Nothing changes here, too. The constructor takes care of creating the correct distributed dof handler.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"dh = DofHandler(dgrid)\npush!(dh, :u, 1, ip)\nclose!(dh);\nnothing #hide","category":"page"},{"location":"examples/heat_equation_hypre/#Boundary-conditions","page":"Distributed Assembly with HYPRE.jl","title":"Boundary conditions","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Nothing has to be changed here either.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"ch = ConstraintHandler(dh);\n∂Ω = union(getfaceset.((dgrid, ), [\"left\", \"right\", \"top\", \"bottom\", \"front\", \"back\"])...);\ndbc = Dirichlet(:u, ∂Ω, (x, t) -> 0)\nadd!(ch, dbc);\nclose!(ch)","category":"page"},{"location":"examples/heat_equation_hypre/#Assembling-the-linear-system","page":"Distributed Assembly with HYPRE.jl","title":"Assembling the linear system","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Assembling the system works also mostly analogue.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"function doassemble(cellvalues::CellValues, dh::FerriteDistributed.NODDofHandler{dim}, ch::ConstraintHandler) where {dim}\n    n_basefuncs = getnbasefunctions(cellvalues)\n    Ke = zeros(n_basefuncs, n_basefuncs)\n    fe = zeros(n_basefuncs)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"––––––––––- Distributed assembly –––––––––– The synchronization with the global sparse matrix is handled by an assembler again. You can choose from different backends, which are described in the docs and will be expaned over time. This call may trigger a large amount of communication.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"TODO how to put this into an interface?","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"    dgrid = getglobalgrid(dh)\n    comm = global_comm(dgrid)\n    ldofrange = local_dof_range(dh)\n    K = HYPREMatrix(comm, first(ldofrange), last(ldofrange))\n    f = HYPREVector(comm, first(ldofrange), last(ldofrange))\n\n    assembler = start_assemble(K, f)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"For the local assembly nothing changes","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"    for cell in CellIterator(dh)\n        fill!(Ke, 0)\n        fill!(fe, 0)\n\n        reinit!(cellvalues, cell)\n        coords = getcoordinates(cell)\n\n        for q_point in 1:getnquadpoints(cellvalues)\n            dΩ = getdetJdV(cellvalues, q_point)\n\n            for i in 1:n_basefuncs\n                v  = shape_value(cellvalues, q_point, i)\n                ∇v = shape_gradient(cellvalues, q_point, i)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Manufactured solution of Π cos(xᵢπ)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"                x = spatial_coordinate(cellvalues, q_point, coords)\n                fe[i] += (π/2)^2 * dim * prod(cos, x*π/2) * v * dΩ\n\n                for j in 1:n_basefuncs\n                    ∇u = shape_gradient(cellvalues, q_point, j)\n                    Ke[i, j] += (∇v ⋅ ∇u) * dΩ\n                end\n            end\n        end\n\n        apply_local!(Ke, fe, celldofs(cell), ch)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"TODO how to put this into an interface.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"        assemble!(assembler, dh.ldof_to_gdof[celldofs(cell)], fe, Ke)\n    end","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Finally, for the HYPREAssembler we have to call end_assemble to construct the global sparse matrix and the global right hand side vector.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"    end_assemble(assembler)\n\n    return K, f\nend\nnothing # hide","category":"page"},{"location":"examples/heat_equation_hypre/#Solution-of-the-system","page":"Distributed Assembly with HYPRE.jl","title":"Solution of the system","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Again, we assemble our problem. Note that we applied the constraints locally.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"K, f = doassemble(cellvalues, dh, ch);\nnothing #hide","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"We use CG with AMG preconditioner to solve the system.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"precond = HYPRE.BoomerAMG()\nsolver = HYPRE.PCG(; Precond = precond)\nuₕ = HYPRE.solve(solver, K, f)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"And convert the solution from HYPRE to Ferrite","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"u_local = Vector{Float64}(undef, FerriteDistributed.num_local_dofs(dh))\nFerriteDistributed.extract_local_part!(u_local, uₕ, dh)","category":"page"},{"location":"examples/heat_equation_hypre/#Exporting-via-PVTK","page":"Distributed Assembly with HYPRE.jl","title":"Exporting via PVTK","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"To visualize the result we export the grid and our field u to a VTK-file, which can be viewed in e.g. ParaView.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"vtk_grid(\"heat_equation_distributed\", dh) do vtk\n    vtk_point_data(vtk, dh, u_local)","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"For debugging purposes it can be helpful to enrich the visualization with some meta  information about the grid and its partitioning","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"    vtk_shared_vertices(vtk, dgrid)\n    vtk_shared_faces(vtk, dgrid)\n    vtk_partitioning(vtk, dgrid)\nend","category":"page"},{"location":"examples/heat_equation_hypre/#distributed-assembly-plain-hypre","page":"Distributed Assembly with HYPRE.jl","title":"Plain program","text":"","category":"section"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"Here follows a version of the program without any comments. The file is also available here: distributed_assembly_hypre.jl.","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"using FerriteDistributed\nusing HYPRE, Metis\n\nimport FerriteDistributed: getglobalgrid, global_comm, local_dof_range #TODO REMOVE THIS\n\nMPI.Init()\nHYPRE.Init()\n\ndgrid = generate_nod_grid(MPI.COMM_WORLD, Hexahedron, (10, 10, 10); partitioning_alg=FerriteDistributed.PartitioningAlgorithm.Metis(:RECURSIVE));\n\nref = RefHexahedron\nip = Lagrange{ref, 2}()\nip_geo = Lagrange{ref, 1}()\nqr = QuadratureRule{ref}(3)\ncellvalues = CellValues(qr, ip, ip_geo);\n\ndh = DofHandler(dgrid)\npush!(dh, :u, 1, ip)\nclose!(dh);\n\nch = ConstraintHandler(dh);\n∂Ω = union(getfaceset.((dgrid, ), [\"left\", \"right\", \"top\", \"bottom\", \"front\", \"back\"])...);\ndbc = Dirichlet(:u, ∂Ω, (x, t) -> 0)\nadd!(ch, dbc);\nclose!(ch)\n\nfunction doassemble(cellvalues::CellValues, dh::FerriteDistributed.NODDofHandler{dim}, ch::ConstraintHandler) where {dim}\n    n_basefuncs = getnbasefunctions(cellvalues)\n    Ke = zeros(n_basefuncs, n_basefuncs)\n    fe = zeros(n_basefuncs)\n\n    dgrid = getglobalgrid(dh)\n    comm = global_comm(dgrid)\n    ldofrange = local_dof_range(dh)\n    K = HYPREMatrix(comm, first(ldofrange), last(ldofrange))\n    f = HYPREVector(comm, first(ldofrange), last(ldofrange))\n\n    assembler = start_assemble(K, f)\n\n    for cell in CellIterator(dh)\n        fill!(Ke, 0)\n        fill!(fe, 0)\n\n        reinit!(cellvalues, cell)\n        coords = getcoordinates(cell)\n\n        for q_point in 1:getnquadpoints(cellvalues)\n            dΩ = getdetJdV(cellvalues, q_point)\n\n            for i in 1:n_basefuncs\n                v  = shape_value(cellvalues, q_point, i)\n                ∇v = shape_gradient(cellvalues, q_point, i)\n\n                x = spatial_coordinate(cellvalues, q_point, coords)\n                fe[i] += (π/2)^2 * dim * prod(cos, x*π/2) * v * dΩ\n\n                for j in 1:n_basefuncs\n                    ∇u = shape_gradient(cellvalues, q_point, j)\n                    Ke[i, j] += (∇v ⋅ ∇u) * dΩ\n                end\n            end\n        end\n\n        apply_local!(Ke, fe, celldofs(cell), ch)\n\n        assemble!(assembler, dh.ldof_to_gdof[celldofs(cell)], fe, Ke)\n    end\n\n    end_assemble(assembler)\n\n    return K, f\nend\n\nK, f = doassemble(cellvalues, dh, ch);\n\nprecond = HYPRE.BoomerAMG()\nsolver = HYPRE.PCG(; Precond = precond)\nuₕ = HYPRE.solve(solver, K, f)\n\nu_local = Vector{Float64}(undef, FerriteDistributed.num_local_dofs(dh))\nFerriteDistributed.extract_local_part!(u_local, uₕ, dh)\n\nvtk_grid(\"heat_equation_distributed\", dh) do vtk\n    vtk_point_data(vtk, dh, u_local)\n\n    vtk_shared_vertices(vtk, dgrid)\n    vtk_shared_faces(vtk, dgrid)\n    vtk_partitioning(vtk, dgrid)\nend","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"","category":"page"},{"location":"examples/heat_equation_hypre/","page":"Distributed Assembly with HYPRE.jl","title":"Distributed Assembly with HYPRE.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"EditURL = \"../literate/heat_equation_pa.jl\"","category":"page"},{"location":"examples/heat_equation_pa/#Distributed-Assembly-with-PartitionedArrays.jl","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"","category":"section"},{"location":"examples/heat_equation_pa/#Introduction","page":"Distributed Assembly with PartitionedArrays.jl","title":"Introduction","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Now we want to solve the heat problem in parallel. To be specific, this example shows how to utilize process parallelism to assemble finite element matrices in parallel. This example presumes that the reader is familiar with solving the heat problem in serial with Ferrite.jl, as presented in the first example.","category":"page"},{"location":"examples/heat_equation_pa/#Commented-Program","page":"Distributed Assembly with PartitionedArrays.jl","title":"Commented Program","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Now we solve the problem in Ferrite. What follows is a program spliced with comments. The full program, without comments, can be found in the next section.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"First we load Ferrite, and some other packages we need","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"using FerriteDistributed\nusing PartitionedArrays, Metis\nusing IterativeSolvers","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Launch MPI","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"MPI.Init()","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"We start generating a simple grid with 20x20 quadrilateral elements and distribute it across our processors using generate_distributed_grid.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"dgrid = generate_nod_grid(MPI.COMM_WORLD, Hexahedron, (10, 10, 10); partitioning_alg=FerriteDistributed.PartitioningAlgorithm.Metis(:RECURSIVE));\nnothing #hide","category":"page"},{"location":"examples/heat_equation_pa/#Trial-and-test-functions","page":"Distributed Assembly with PartitionedArrays.jl","title":"Trial and test functions","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Nothing changes here.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"ref = RefHexahedron\nip = Lagrange{ref, 2}()\nip_geo = Lagrange{ref, 1}()\nqr = QuadratureRule{ref}(3)\ncellvalues = CellValues(qr, ip, ip_geo);\nnothing #hide","category":"page"},{"location":"examples/heat_equation_pa/#Degrees-of-freedom","page":"Distributed Assembly with PartitionedArrays.jl","title":"Degrees of freedom","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Nothing changes here, too. The constructor takes care of creating the correct distributed dof handler.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"dh = DofHandler(dgrid)\nadd!(dh, :u, ip)\nclose!(dh);\nnothing #hide","category":"page"},{"location":"examples/heat_equation_pa/#Boundary-conditions","page":"Distributed Assembly with PartitionedArrays.jl","title":"Boundary conditions","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Nothing has to be changed here either.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"ch = ConstraintHandler(dh);\n∂Ω = union(getfaceset.((dgrid, ), [\"left\", \"right\", \"top\", \"bottom\", \"front\", \"back\"])...);\ndbc = Dirichlet(:u, ∂Ω, (x, t) -> 0)\nadd!(ch, dbc);\nclose!(ch)\nupdate!(ch, 0.0);\nnothing #hide","category":"page"},{"location":"examples/heat_equation_pa/#Assembling-the-linear-system","page":"Distributed Assembly with PartitionedArrays.jl","title":"Assembling the linear system","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Assembling the system works also mostly analogue. Note that the dof handler type changed.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"function doassemble(cellvalues::CellValues, dh::FerriteDistributed.NODDofHandler{dim}) where {dim}\n    n_basefuncs = getnbasefunctions(cellvalues)\n    Ke = zeros(n_basefuncs, n_basefuncs)\n    fe = zeros(n_basefuncs)","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"––––––––––- Distributed assembly –––––––––– The synchronization with the global sparse matrix is handled by an assembler again. You can choose from different backends, which are described in the docs and will be expaned over time. This call may trigger a large amount of communication. NOTE: At the time of writing the only backend available is a COO       assembly via PartitionedArrays.jl .","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"    assembler = start_assemble(dh, distribute_with_mpi(LinearIndices((MPI.Comm_size(MPI.COMM_WORLD),))))","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"For the local assembly nothing changes","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"    for cell in CellIterator(dh)\n        fill!(Ke, 0)\n        fill!(fe, 0)\n\n        reinit!(cellvalues, cell)\n        coords = getcoordinates(cell)\n\n        for q_point in 1:getnquadpoints(cellvalues)\n            dΩ = getdetJdV(cellvalues, q_point)\n\n            for i in 1:n_basefuncs\n                v  = shape_value(cellvalues, q_point, i)\n                ∇v = shape_gradient(cellvalues, q_point, i)","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Manufactured solution of Π cos(xᵢπ)","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"                x = spatial_coordinate(cellvalues, q_point, coords)\n                fe[i] += (π/2)^2 * dim * prod(cos, x*π/2) * v * dΩ\n\n                for j in 1:n_basefuncs\n                    ∇u = shape_gradient(cellvalues, q_point, j)\n                    Ke[i, j] += (∇v ⋅ ∇u) * dΩ\n                end\n            end\n        end","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Note that this call should be communication-free!","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"        Ferrite.assemble!(assembler, celldofs(cell), fe, Ke)\n    end","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Finally, for the PartitionedArraysCOOAssembler we have to call end_assemble to construct the global sparse matrix and the global right hand side vector.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"    return end_assemble(assembler)\nend\nnothing # hide","category":"page"},{"location":"examples/heat_equation_pa/#Solution-of-the-system","page":"Distributed Assembly with PartitionedArrays.jl","title":"Solution of the system","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Again, we assemble our problem and apply the constraints as needed.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"K, f = doassemble(cellvalues, dh);\napply!(K, f, ch)","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"To compute the solution we utilize conjugate gradients because at the time of writing this is the only available scalable working solver. Additional note: At the moment of writing this we have no good preconditioners for PSparseMatrix in Julia, partly due to unimplemented multiplication operators for the matrix data type.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"u = cg(K, f)","category":"page"},{"location":"examples/heat_equation_pa/#Exporting-via-PVTK","page":"Distributed Assembly with PartitionedArrays.jl","title":"Exporting via PVTK","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"To visualize the result we export the grid and our field u to a VTK-file, which can be viewed in e.g. ParaView.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"vtk_grid(\"heat_equation_distributed\", dh) do vtk\n    vtk_point_data(vtk, dh, u)","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"For debugging purposes it can be helpful to enrich the visualization with some meta  information about the grid and its partitioning","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"    vtk_shared_vertices(vtk, dgrid)\n    vtk_shared_faces(vtk, dgrid)\n    vtk_partitioning(vtk, dgrid)\nend","category":"page"},{"location":"examples/heat_equation_pa/#distributed-assembly-plain-pa","page":"Distributed Assembly with PartitionedArrays.jl","title":"Plain program","text":"","category":"section"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"Here follows a version of the program without any comments. The file is also available here: distributed_assembly_pa.jl.","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"using FerriteDistributed\nusing PartitionedArrays, Metis\nusing IterativeSolvers\n\nMPI.Init()\n\ndgrid = generate_nod_grid(MPI.COMM_WORLD, Hexahedron, (10, 10, 10); partitioning_alg=FerriteDistributed.PartitioningAlgorithm.Metis(:RECURSIVE));\n\nref = RefHexahedron\nip = Lagrange{ref, 2}()\nip_geo = Lagrange{ref, 1}()\nqr = QuadratureRule{ref}(3)\ncellvalues = CellValues(qr, ip, ip_geo);\n\ndh = DofHandler(dgrid)\nadd!(dh, :u, ip)\nclose!(dh);\n\nch = ConstraintHandler(dh);\n∂Ω = union(getfaceset.((dgrid, ), [\"left\", \"right\", \"top\", \"bottom\", \"front\", \"back\"])...);\ndbc = Dirichlet(:u, ∂Ω, (x, t) -> 0)\nadd!(ch, dbc);\nclose!(ch)\nupdate!(ch, 0.0);\n\nfunction doassemble(cellvalues::CellValues, dh::FerriteDistributed.NODDofHandler{dim}) where {dim}\n    n_basefuncs = getnbasefunctions(cellvalues)\n    Ke = zeros(n_basefuncs, n_basefuncs)\n    fe = zeros(n_basefuncs)\n\n    assembler = start_assemble(dh, distribute_with_mpi(LinearIndices((MPI.Comm_size(MPI.COMM_WORLD),))))\n\n    for cell in CellIterator(dh)\n        fill!(Ke, 0)\n        fill!(fe, 0)\n\n        reinit!(cellvalues, cell)\n        coords = getcoordinates(cell)\n\n        for q_point in 1:getnquadpoints(cellvalues)\n            dΩ = getdetJdV(cellvalues, q_point)\n\n            for i in 1:n_basefuncs\n                v  = shape_value(cellvalues, q_point, i)\n                ∇v = shape_gradient(cellvalues, q_point, i)\n\n                x = spatial_coordinate(cellvalues, q_point, coords)\n                fe[i] += (π/2)^2 * dim * prod(cos, x*π/2) * v * dΩ\n\n                for j in 1:n_basefuncs\n                    ∇u = shape_gradient(cellvalues, q_point, j)\n                    Ke[i, j] += (∇v ⋅ ∇u) * dΩ\n                end\n            end\n        end\n\n        Ferrite.assemble!(assembler, celldofs(cell), fe, Ke)\n    end\n\n    return end_assemble(assembler)\nend\n\nK, f = doassemble(cellvalues, dh);\napply!(K, f, ch)\n\nu = cg(K, f)\n\nvtk_grid(\"heat_equation_distributed\", dh) do vtk\n    vtk_point_data(vtk, dh, u)\n\n    vtk_shared_vertices(vtk, dgrid)\n    vtk_shared_faces(vtk, dgrid)\n    vtk_partitioning(vtk, dgrid)\nend","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"","category":"page"},{"location":"examples/heat_equation_pa/","page":"Distributed Assembly with PartitionedArrays.jl","title":"Distributed Assembly with PartitionedArrays.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/interface/#Interface","page":"Interface","title":"Interface","text":"","category":"section"},{"location":"reference/interface/","page":"Interface","title":"Interface","text":"get_shared_vertices\nget_shared_vertex\nget_shared_edges\nget_shared_edge\nget_shared_faces\nget_shared_face\nis_shared_vertex\nis_shared_edge\nis_shared_face\ngetlocalgrid","category":"page"},{"location":"reference/interface/#FerriteDistributed.get_shared_vertices","page":"Interface","title":"FerriteDistributed.get_shared_vertices","text":"get_shared_vertices(::AbstractNODGrid)\n\nGet an interable over the shared vertices.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.get_shared_vertex","page":"Interface","title":"FerriteDistributed.get_shared_vertex","text":"get_shared_vertex(::AbstractNODGrid, ::VertexIndex)\n\nGet the shared vertex associated to the VertexIndex, if it exists.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.get_shared_edges","page":"Interface","title":"FerriteDistributed.get_shared_edges","text":"getsharededges(::AbstractNODGrid)\n\nGet an interable over the shared edges.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.get_shared_edge","page":"Interface","title":"FerriteDistributed.get_shared_edge","text":"get_shared_edge(::AbstractNODGrid, ::EdgeIndex)\n\nGet the shared edge associated to the EdgeIndex, if it exists.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.get_shared_faces","page":"Interface","title":"FerriteDistributed.get_shared_faces","text":"get_shared_faces(::AbstractNODGrid)\n\nGet an interable over the shared faces.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.get_shared_face","page":"Interface","title":"FerriteDistributed.get_shared_face","text":"get_shared_face(::AbstractNODGrid, ::FaceIndex)\n\nGet the shared edge associated to the FaceIndex, if it exists.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.is_shared_vertex","page":"Interface","title":"FerriteDistributed.is_shared_vertex","text":"is_shared_vertex(::AbstractNODGrid, ::VertexIndex)\n\nCheck if a VertexIndex is a shared vertex.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.is_shared_edge","page":"Interface","title":"FerriteDistributed.is_shared_edge","text":"is_shared_edge(::AbstractNODGrid, ::EdgeIndex)\n\nCheck if a EdgeIndex is a shared edge.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.is_shared_face","page":"Interface","title":"FerriteDistributed.is_shared_face","text":"is_shared_face(::AbstractNODGrid, ::FaceIndex)\n\nCheck if a FaceIndex is a shared face.\n\n\n\n\n\n","category":"function"},{"location":"reference/interface/#FerriteDistributed.getlocalgrid","page":"Interface","title":"FerriteDistributed.getlocalgrid","text":"getlocalgrid(::AbstractNODGrid)\n\nGet the representative local grid containing only a vanilla local grid.\n\n\n\n\n\n","category":"function"},{"location":"reference/grid/","page":"NODGrid","title":"NODGrid","text":"CurrentModule = FerriteDistributed\nDocTestSetup = :(using FerriteDistributed)","category":"page"},{"location":"reference/grid/#NODGrid","page":"NODGrid","title":"NODGrid","text":"","category":"section"},{"location":"reference/grid/","page":"NODGrid","title":"NODGrid","text":"NODGrid\nNODGrid(::MPI.Comm, ::Grid)\nNODGrid(::MPI.Comm, ::Grid, ::CoverTopology, ::Vector{Int})\nFerriteDistributed.SharedEntity\nFerriteDistributed.SharedVertex\nFerriteDistributed.SharedFace\nFerriteDistributed.SharedEdge\nFerriteDistributed.remote_entities\nFerriteDistributed.global_comm\nFerriteDistributed.interface_comm\nFerriteDistributed.global_rank\nFerriteDistributed.global_nranks","category":"page"},{"location":"reference/grid/#FerriteDistributed.NODGrid","page":"NODGrid","title":"FerriteDistributed.NODGrid","text":"NODGrid{dim,C<:AbstractCell,T<:Real} <: AbstractNODGrid{dim}\n\nScalable non-overlapping distributed grid. This data structure is composed of a local_grid and full topological information on the process boundary, i.e. how vertices, edges and faces are connectedted between processes.\n\ntodo: Todo\nPartitionedArrays.jl ready constructor via extension\n\n\n\n\n\n","category":"type"},{"location":"reference/grid/#FerriteDistributed.NODGrid-Tuple{MPI.Comm, Grid}","page":"NODGrid","title":"FerriteDistributed.NODGrid","text":"NODGrid(grid_comm::MPI.Comm, grid_to_distribute::Grid{dim,C,T})\n\nConstruct a non-overlapping distributed grid from a grid with the SFC induced by the element ordering on a specified MPI communicator. It is assumed that this function is called with exactly the same grid on each MPI process in the communicator.\n\n\n\n\n\n","category":"method"},{"location":"reference/grid/#FerriteDistributed.NODGrid-Tuple{MPI.Comm, Grid, FerriteDistributed.CoverTopology, Vector{Int64}}","page":"NODGrid","title":"FerriteDistributed.NODGrid","text":"NODGrid(grid_comm::MPI.Comm, grid_to_distribute::Grid{dim,C,T}, grid_topology::CoverTopology, partitioning::Vector{<:Integer})\n\nConstruct a non-overlapping distributed grid from a grid with given topology and partitioning on a specified MPI communicator.\n\n\n\n\n\n","category":"method"},{"location":"reference/grid/#FerriteDistributed.SharedEntity","page":"NODGrid","title":"FerriteDistributed.SharedEntity","text":"SharedEntity\n\nSupertype for shared entities.\n\n\n\n\n\n","category":"type"},{"location":"reference/grid/#FerriteDistributed.SharedVertex","page":"NODGrid","title":"FerriteDistributed.SharedVertex","text":"SharedVertex <: SharedEntity\n\nA shared vertex induced by a local vertex index and all remote vertex indices on all remote ranks.\n\n\n\n\n\n","category":"type"},{"location":"reference/grid/#FerriteDistributed.SharedFace","page":"NODGrid","title":"FerriteDistributed.SharedFace","text":"SharedFace <: SharedEntity\n\nA shared face induced by a local face index and all remote face indices on all remote ranks.\n\n\n\n\n\n","category":"type"},{"location":"reference/grid/#FerriteDistributed.SharedEdge","page":"NODGrid","title":"FerriteDistributed.SharedEdge","text":"SharedEdge <: SharedEntity\n\nA shared edge induced by a local edge index and all remote edge indices on all remote ranks.\n\n\n\n\n\n","category":"type"},{"location":"reference/grid/#FerriteDistributed.remote_entities","page":"NODGrid","title":"FerriteDistributed.remote_entities","text":"remote_entities(::SharedEntity)\n\nGet an iterable of pairs, containing the 1-based rank and a collection of EntityIndices on the rank.\n\n\n\n\n\n","category":"function"},{"location":"reference/grid/#FerriteDistributed.global_comm","page":"NODGrid","title":"FerriteDistributed.global_comm","text":"global_comm(::AbstractNODGrid)\n\nGet a communicator handle for the full grid.\n\n\n\n\n\nglobal_comm(::NODGrid)\n\nGlobal dense communicator of the distributed grid.\n\n\n\n\n\n","category":"function"},{"location":"reference/grid/#FerriteDistributed.interface_comm","page":"NODGrid","title":"FerriteDistributed.interface_comm","text":"interface_comm(::AbstractNODGrid)\n\nGet a graph comminicator handle for the process boundary.\n\n\n\n\n\ninterface_comm(::NODGrid)\n\nGraph communicator for shared vertices. Guaranteed to be derived from the communicator  returned by @global_comm .\n\n\n\n\n\n","category":"function"},{"location":"reference/grid/#FerriteDistributed.global_rank","page":"NODGrid","title":"FerriteDistributed.global_rank","text":"global_rank(::AbstractNODGrid)\n\nGet the 1-based rank of the global communicator of the grid.\n\n\n\n\n\nglobal_rank(::NODGrid)\n\nGet the rank on the global communicator of the distributed grid.\n\n\n\n\n\n","category":"function"},{"location":"reference/grid/#FerriteDistributed.global_nranks","page":"NODGrid","title":"FerriteDistributed.global_nranks","text":"global_nranks(::AbstractNODGrid)\n\nGet the number of ranks of the global communicator of the grid.\n\n\n\n\n\nglobal_nranks(::NODGrid)\n\nGet the number of ranks on the global communicator of the distributed grid.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"DocTestSetup = :(using FerriteDistributed)","category":"page"},{"location":"#FerriteDistributed.jl","page":"Home","title":"FerriteDistributed.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Non-overlapping distributed grids for Ferrite.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis package is still experimental and may break!","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The usage of this package is straight forward. Either generate the non-overlapping distributed grid directly via","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MPI\nusing FerriteDistributed\nMPI.Init()\ndgrid = generate_nod_grid(MPI.COMM_WORLD, Quadrilateral, (100, 100))","category":"page"},{"location":"","page":"Home","title":"Home","text":"or distribute your own grid","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MPI\nusing Ferrite, FerriteDistributed\nMPI.Init()\ngrid = ...\ndgrid = NODGrid(MPI.COMM_WORLD, grid, FerriteDistributed.PartitioningAlgorithm.SFC())","category":"page"},{"location":"","page":"Home","title":"Home","text":"for more details on the calls please consult the API docs.","category":"page"},{"location":"#Debugging-Information","page":"Home","title":"Debugging Information","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The debug mode for this package is tied to the debug mode of Ferrite. Hence, it can be turned on via","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Ferrite\nFerrite.debug_mode()","category":"page"},{"location":"","page":"Home","title":"Home","text":"followed by restarting the Julia process. It can be turned off again by calling","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Ferrite\nFerrite.debug_mode(enable=false)","category":"page"},{"location":"","page":"Home","title":"Home","text":"also followed by restarting the Julia process.","category":"page"},{"location":"reference/utils/#Utils","page":"Utils","title":"Utils","text":"","category":"section"},{"location":"reference/utils/","page":"Utils","title":"Utils","text":"generate_nod_grid\ncreate_partitioning\ncompute_owner\nvtk_shared_edges\nvtk_shared_faces\nvtk_shared_vertices\nvtk_partitioning","category":"page"},{"location":"reference/utils/#FerriteDistributed.generate_nod_grid","page":"Utils","title":"FerriteDistributed.generate_nod_grid","text":"generate_nod_grid(comm::MPI.Comm, args...)\n\nHelper to directly generate non-overlapping distributed grids, designed to replace the call to generate_grid for use in distributed environments.\n\n\n\n\n\n","category":"function"},{"location":"reference/utils/#FerriteDistributed.create_partitioning","page":"Utils","title":"FerriteDistributed.create_partitioning","text":"create_partitioning(::Ferrite.AbstractGrid, ::Ferrite.AbstractTopology, nparts::Int, partition_alg)::Vector{Int}\n\nInternal entry point for the construction of Ferrite.jl grid partitionings. Creates a vector where the index corresponds to the cell index of the input grid and the value is the 1-based rank.\n\n\n\n\n\n","category":"function"},{"location":"reference/utils/#FerriteDistributed.compute_owner","page":"Utils","title":"FerriteDistributed.compute_owner","text":"compute_owner(::AbstractNODGrid, ::SharedEntity)\n\nCompute which rank (1-based index) owns the corresponding entity. \n\n\n\n\n\n","category":"function"},{"location":"reference/utils/#FerriteDistributed.vtk_shared_edges","page":"Utils","title":"FerriteDistributed.vtk_shared_edges","text":"Enrich the VTK file with meta information about shared edges.\n\n\n\n\n\n","category":"function"},{"location":"reference/utils/#FerriteDistributed.vtk_shared_faces","page":"Utils","title":"FerriteDistributed.vtk_shared_faces","text":"Enrich the VTK file with meta information about shared faces.\n\n\n\n\n\n","category":"function"},{"location":"reference/utils/#FerriteDistributed.vtk_shared_vertices","page":"Utils","title":"FerriteDistributed.vtk_shared_vertices","text":"Enrich the VTK file with meta information about shared vertices.\n\n\n\n\n\n","category":"function"},{"location":"reference/utils/#FerriteDistributed.vtk_partitioning","page":"Utils","title":"FerriteDistributed.vtk_partitioning","text":"Enrich the VTK file with partitioning meta information.\n\n\n\n\n\n","category":"function"}]
}
